{"nbformat_minor": 2, "cells": [{"execution_count": 1, "cell_type": "code", "source": "spark\n#loads spark", "outputs": [{"execution_count": 1, "output_type": "execute_result", "data": {"text/plain": "<pyspark.sql.session.SparkSession at 0x7fc7bd147090>", "text/html": "\n            <div>\n                <p><b>SparkSession - hive</b></p>\n                \n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"http://pyspark-project-m.us-central1-a.c.essential-aleph-265217.internal:4040\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v2.3.4</code></dd>\n              <dt>Master</dt>\n                <dd><code>yarn</code></dd>\n              <dt>AppName</dt>\n                <dd><code>PySparkShell</code></dd>\n            </dl>\n        </div>\n        \n            </div>\n        "}, "metadata": {}}], "metadata": {}}, {"execution_count": 2, "cell_type": "code", "source": "sc\n#loads spark context: main entry point for Spark functionality. \n#Represents the connection to a spark cluster and can be used to create RDDs \n#https://spark.apache.org/docs/latest/api/java/org/apache/spark/SparkContext.html", "outputs": [{"execution_count": 2, "output_type": "execute_result", "data": {"text/plain": "<SparkContext master=yarn appName=PySparkShell>", "text/html": "\n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"http://pyspark-project-m.us-central1-a.c.essential-aleph-265217.internal:4040\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v2.3.4</code></dd>\n              <dt>Master</dt>\n                <dd><code>yarn</code></dd>\n              <dt>AppName</dt>\n                <dd><code>PySparkShell</code></dd>\n            </dl>\n        </div>\n        "}, "metadata": {}}], "metadata": {}}, {"execution_count": 3, "cell_type": "code", "source": "from pyspark import SparkContext\nsc = SparkContext.getOrCreate()", "outputs": [], "metadata": {}}, {"execution_count": 4, "cell_type": "code", "source": "from pyspark.sql import SparkSession\nspark = SparkSession.builder.getOrCreate()\n#main entry point for dataframe and SQL functionality\n#https://spark.apache.org/docs/latest/api/python/pyspark.sql.html?highlight=sparksession", "outputs": [], "metadata": {}}, {"execution_count": 5, "cell_type": "code", "source": "df = spark.read.csv('gs://dataproc-staging-us-central1-18725869504-rzbbn2fq/QueryResults_Shrode.csv')\ntype(df)\n#get data from bucket (google storage)\n#look at data type\n#data originally from stack overflow data explorer", "outputs": [{"execution_count": 5, "output_type": "execute_result", "data": {"text/plain": "pyspark.sql.dataframe.DataFrame"}, "metadata": {}}], "metadata": {}}, {"execution_count": 6, "cell_type": "code", "source": "#make sure data loaded properly -- remember the data load doesnt't run until we run .take or .collect or save\ndf.take(5)", "outputs": [{"execution_count": 6, "output_type": "execute_result", "data": {"text/plain": "[Row(_c0=u'Id', _c1=u'Tags'),\n Row(_c0=u'41918662', _c1=u'amazon-web-services,docker,kubernetes'),\n Row(_c0=u'41918667', _c1=u'python,r'),\n Row(_c0=u'41918669', _c1=u'restler'),\n Row(_c0=u'41918671', _c1=u'php,preg-match')]"}, "metadata": {}}], "metadata": {}}, {"execution_count": 7, "cell_type": "code", "source": "df.show(5)", "outputs": [{"output_type": "stream", "name": "stdout", "text": "+--------+--------------------+\n|     _c0|                 _c1|\n+--------+--------------------+\n|      Id|                Tags|\n|41918662|amazon-web-servic...|\n|41918667|            python,r|\n|41918669|             restler|\n|41918671|      php,preg-match|\n+--------+--------------------+\nonly showing top 5 rows\n\n"}], "metadata": {}}, {"execution_count": 8, "cell_type": "code", "source": "rdd = df.rdd\ntype(rdd)", "outputs": [{"execution_count": 8, "output_type": "execute_result", "data": {"text/plain": "pyspark.rdd.RDD"}, "metadata": {}}], "metadata": {}}, {"execution_count": 9, "cell_type": "code", "source": "rdd.take(10)", "outputs": [{"execution_count": 9, "output_type": "execute_result", "data": {"text/plain": "[Row(_c0=u'Id', _c1=u'Tags'),\n Row(_c0=u'41918662', _c1=u'amazon-web-services,docker,kubernetes'),\n Row(_c0=u'41918667', _c1=u'python,r'),\n Row(_c0=u'41918669', _c1=u'restler'),\n Row(_c0=u'41918671', _c1=u'php,preg-match'),\n Row(_c0=u'41918674', _c1=u'javascript,node.js,express,ejs'),\n Row(_c0=u'41918675', _c1=u'php,codeigniter,ccavenue'),\n Row(_c0=u'41918686', _c1=u'javascript,angular,webpack,webpack-2,html-webpack-plugin'),\n Row(_c0=u'41918687', _c1=u'javascript,node.js,phpstorm,webstorm,jetbrains-ide'),\n Row(_c0=u'41918690', _c1=u'c,stack,dynamic-memory-allocation,realloc')]"}, "metadata": {}}], "metadata": {}}, {"execution_count": 10, "cell_type": "code", "source": "no_null = rdd.filter(lambda x: x._c0 !='' and x._c1 !='') ", "outputs": [], "metadata": {}}, {"execution_count": 11, "cell_type": "code", "source": "no_null.take(5)", "outputs": [{"execution_count": 11, "output_type": "execute_result", "data": {"text/plain": "[Row(_c0=u'Id', _c1=u'Tags'),\n Row(_c0=u'41918662', _c1=u'amazon-web-services,docker,kubernetes'),\n Row(_c0=u'41918667', _c1=u'python,r'),\n Row(_c0=u'41918669', _c1=u'restler'),\n Row(_c0=u'41918671', _c1=u'php,preg-match')]"}, "metadata": {}}], "metadata": {}}, {"execution_count": 26, "cell_type": "code", "source": "def split_strip(x):\n    return (x._c0.strip(), x._c1.split(','))\n#.strip() strips whitespaces, .split(,) splits on commas", "outputs": [], "metadata": {}}, {"execution_count": 13, "cell_type": "code", "source": "no_null.map(split_strip).take(5)", "outputs": [{"execution_count": 13, "output_type": "execute_result", "data": {"text/plain": "[(u'Id', [u'Tags']),\n (u'41918662', [u'amazon-web-services', u'docker', u'kubernetes']),\n (u'41918667', [u'python', u'r']),\n (u'41918669', [u'restler']),\n (u'41918671', [u'php', u'preg-match'])]"}, "metadata": {}}], "metadata": {}}, {"execution_count": 14, "cell_type": "code", "source": "split_id = no_null.map(split_strip)", "outputs": [], "metadata": {}}, {"execution_count": 27, "cell_type": "code", "source": "#MAP\ntag_to_id = split_id.flatMap(lambda x: [(tag, x[0]) for tag in x[1]])\n#basically a looping option 'for each tag in column 1, do tag, x[0]'", "outputs": [], "metadata": {}}, {"execution_count": 16, "cell_type": "code", "source": "tag_to_id.take(10)", "outputs": [{"execution_count": 16, "output_type": "execute_result", "data": {"text/plain": "[(u'Tags', u'Id'),\n (u'amazon-web-services', u'41918662'),\n (u'docker', u'41918662'),\n (u'kubernetes', u'41918662'),\n (u'python', u'41918667'),\n (u'r', u'41918667'),\n (u'restler', u'41918669'),\n (u'php', u'41918671'),\n (u'preg-match', u'41918671'),\n (u'javascript', u'41918674')]"}, "metadata": {}}], "metadata": {}}, {"execution_count": 17, "cell_type": "code", "source": "#REDUCE\ngrouped_by_key = tag_to_id.groupByKey()\ngrouped_by_key.take(5)", "outputs": [{"execution_count": 17, "output_type": "execute_result", "data": {"text/plain": "[(u'h.264', <pyspark.resultiterable.ResultIterable at 0x7fc7bc0abb90>),\n (u'biopython', <pyspark.resultiterable.ResultIterable at 0x7fc7bc0abbd0>),\n (u'screen-resolution',\n  <pyspark.resultiterable.ResultIterable at 0x7fc7bc0abc50>),\n (u'nsstackview', <pyspark.resultiterable.ResultIterable at 0x7fc7bc0abc90>),\n (u'userscripts', <pyspark.resultiterable.ResultIterable at 0x7fc7bc0abcd0>)]"}, "metadata": {}}], "metadata": {}}, {"execution_count": 18, "cell_type": "code", "source": "#FINISH INDEX\ninverted_index = grouped_by_key.map(lambda x: (x[0], list(x[1])))", "outputs": [], "metadata": {}}, {"execution_count": 19, "cell_type": "code", "source": "#inverted_index.collect()", "outputs": [], "metadata": {}}, {"execution_count": 20, "cell_type": "code", "source": "inverted_index.take(5)", "outputs": [{"execution_count": 20, "output_type": "execute_result", "data": {"text/plain": "[(u'h.264',\n  [u'45691558',\n   u'43218009',\n   u'45816914',\n   u'45933221',\n   u'42300009',\n   u'42319421',\n   u'45066008',\n   u'41470525']),\n (u'biopython', [u'43216346', u'46990284', u'45869094']),\n (u'screen-resolution', [u'43345440', u'41468537']),\n (u'nsstackview', [u'46009170']),\n (u'userscripts', [u'42319639'])]"}, "metadata": {}}], "metadata": {}}, {"execution_count": 21, "cell_type": "code", "source": "#Write data to disk\n    #We can use saveAsTextFile function, which saves the data in a folder similar to when hadoop outputs it's mapreduce files", "outputs": [], "metadata": {}}, {"execution_count": 22, "cell_type": "code", "source": "#Import note: Just like with HDFS, the output directory we write to cannot already exist. \n#So if you run this next cell once and want to run it again, you'll have to delete the folder from the bucket. \n#This can be either through the GUI or using the Google's API. In the GUI you'll have to refresh the page (or click the refresh button)\n#to see any new files/folders.", "outputs": [], "metadata": {}}, {"execution_count": 23, "cell_type": "code", "source": "from google.cloud import storage\nclient = storage.Client()\n# https://console.cloud.google.com/storage/browser/[bucket-id]/\n# bucket = client.get_bucket('bucket-id-here')\n# command to delete the folder here", "outputs": [], "metadata": {}}, {"execution_count": 24, "cell_type": "code", "source": "#remember we will write to our google bucket for our cluster.", "outputs": [], "metadata": {}}, {"execution_count": 29, "cell_type": "code", "source": "inverted_index.saveAsTextFile('gs://dataproc-staging-us-central1-18725869504-rzbbn2fq/Shrode_inverted_index.csv')", "outputs": [], "metadata": {}}, {"execution_count": 25, "cell_type": "code", "source": "#notice the output format in the folder is like HDFS, with files broken up in parts. ", "outputs": [], "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "", "outputs": [], "metadata": {}}], "nbformat": 4, "metadata": {"kernelspec": {"display_name": "PySpark", "name": "pyspark", "language": "python"}, "language_info": {"mimetype": "text/x-python", "nbconvert_exporter": "python", "version": "2.7.14", "name": "python", "file_extension": ".py", "pygments_lexer": "ipython2", "codemirror_mode": {"version": 2, "name": "ipython"}}}}